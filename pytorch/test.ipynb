{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2650d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15a71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDownSample(torch.nn.Module):\n",
    "    def __init__(self,downscale,in_channel,reduce):\n",
    "        super().__init__()\n",
    "        self.downscale = downscale\n",
    "        self.Q = torch.nn.Linear(in_channel, in_channel//reduce, bias=False)\n",
    "        self.K = torch.nn.Linear(in_channel, in_channel//reduce, bias=False)\n",
    "        \n",
    "    def forward(self,fm):\n",
    "        B,C,H,W = fm.shape\n",
    "        new_H = H//self.downscale\n",
    "        new_W = W//self.downscale\n",
    "        \n",
    "        fm = torch.reshape(fm,[B, C, new_H, self.downscale, new_W, self.downscale])\n",
    "        fm = torch.permute(fm,[0,2,4,3,5,1])\n",
    "        fm = torch.reshape(fm,[B, new_H, new_W, self.downscale*self.downscale, C])\n",
    "        \n",
    "        q = torch.mean(fm,axis=-2,keepdim=True)\n",
    "        q = self.Q(q)\n",
    "        q = q / q.shape[-1]**0.5\n",
    "        \n",
    "        k = self.K(fm)\n",
    "        \n",
    "        qk = torch.matmul(q,torch.transpose(k,-2,-1))\n",
    "        qk = torch.softmax(qk, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(qk,fm)\n",
    "        out = torch.squeeze(out,dim=-2)\n",
    "        out = torch.permute(out,[0,3,1,2])\n",
    "        return out\n",
    "    \n",
    "class Stem(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,4,7,padding='same'),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            torch.nn.Conv2d(4,7,3,padding='same'),\n",
    "            torch.nn.Mish(inplace=True)\n",
    "            )\n",
    "        self.merge = torch.nn.Conv2d(8,8,3,padding='same')\n",
    "    \n",
    "    def forward(self,image):\n",
    "        x = self.body(image)\n",
    "        x = torch.cat((x,image),dim=1)\n",
    "        x = self.merge(x)\n",
    "        return x\n",
    "    \n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self,in_channel,reduce,n_subblocks):\n",
    "        super().__init__()\n",
    "        self.fn = torch.nn.Sequential(*self.create_subblocks(in_channel,reduce,n_subblocks))\n",
    "        self.att_inp = torch.nn.Conv2d(in_channel,in_channel//reduce,1,bias=False)\n",
    "        self.att_x = torch.nn.Conv2d(in_channel,in_channel//reduce,1,bias=False)\n",
    "    \n",
    "    def create_subblocks(self,in_channel,reduce,n):\n",
    "        return [torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channel,in_channel//reduce,1,bias=False),\n",
    "            torch.nn.Conv2d(in_channel//reduce,in_channel//reduce,3,padding='same',groups=in_channel//reduce),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            torch.nn.Conv2d(in_channel//reduce,in_channel,1),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            ) for _ in range(n)]\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        x = self.fn(inp)\n",
    "        att = torch.sigmoid(torch.mean(self.att_inp(inp)*self.att_x(x),dim=1,keepdim=True))\n",
    "        x = x*att + inp*(1-att)\n",
    "        return x\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = Stem() # 512*512 # 8\n",
    "        self.block0 = ResBlock(in_channel=8,reduce=2,n_subblocks=5) # 512*512 # 8\n",
    "        \n",
    "        self.block1 = torch.nn.Sequential( # 64*64 # 16\n",
    "            AttentionDownSample(downscale=8,in_channel=8,reduce=2),\n",
    "            torch.nn.Conv2d(8,16,1,bias=False),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            ResBlock(in_channel=16,reduce=2,n_subblocks=4)\n",
    "            )\n",
    "        self.fms_reduce_1 = torch.nn.Conv2d(16, 4, 1, bias=False)\n",
    "        \n",
    "        self.block2 = torch.nn.Sequential( # 16*16 # 32\n",
    "            AttentionDownSample(downscale=4,in_channel=16,reduce=2),\n",
    "            torch.nn.Conv2d(16,32,1,bias=False),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            ResBlock(in_channel=32,reduce=2,n_subblocks=3)\n",
    "            )\n",
    "        self.fms_reduce_2 = torch.nn.Conv2d(32, 8, 1, bias=False)\n",
    "        \n",
    "        self.block3 = torch.nn.Sequential( # 8*8 # 64\n",
    "            AttentionDownSample(downscale=2,in_channel=32,reduce=2),\n",
    "            torch.nn.Conv2d(32,64,1,bias=False),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            ResBlock(in_channel=64,reduce=2,n_subblocks=2)\n",
    "            )\n",
    "        self.fms_reduce_3 = torch.nn.Conv2d(64, 16, 1, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        fms = []\n",
    "        x = self.stem(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        fms.append(self.fms_reduce_1(x))\n",
    "        x = self.block2(x)\n",
    "        fms.append(self.fms_reduce_2(x))\n",
    "        x = self.block3(x)\n",
    "        fms.append(self.fms_reduce_3(x))\n",
    "        return fms\n",
    "        \n",
    "        \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4+8+16, 32, 1),\n",
    "            torch.nn.Mish(inplace=True),\n",
    "            torch.nn.Conv2d(32, 8, 1),\n",
    "            torch.nn.Mish(inplace=True)\n",
    "            )\n",
    "        \n",
    "    def forward(self,fms):\n",
    "        for i in range(len(fms)):\n",
    "            fms[i] = torch.nn.functional.interpolate(fms[i],size=(512,512),mode='bilinear',align_corners=True)\n",
    "        fms = torch.cat(fms,dim=1)\n",
    "        fms = self.body(fms)\n",
    "        return fms\n",
    "\n",
    "class Backbone(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adfe151",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn((2,1,512,512))\n",
    "x = x.to(device)\n",
    "\n",
    "layer = Backbone()\n",
    "layer = layer.to(device)\n",
    "\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8232ee7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12639e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "Backbone                                      --                        --\n",
      "├─Encoder: 1-1                                [2, 4, 64, 64]            --\n",
      "│    └─Stem: 2-1                              [2, 8, 512, 512]          --\n",
      "│    │    └─Sequential: 3-1                   [2, 7, 512, 512]          459\n",
      "│    │    └─Conv2d: 3-2                       [2, 8, 512, 512]          584\n",
      "│    └─ResBlock: 2-2                          [2, 8, 512, 512]          --\n",
      "│    │    └─Sequential: 3-3                   [2, 8, 512, 512]          560\n",
      "│    │    └─Conv2d: 3-4                       [2, 4, 512, 512]          32\n",
      "│    │    └─Conv2d: 3-5                       [2, 4, 512, 512]          32\n",
      "│    └─Sequential: 2-3                        [2, 16, 64, 64]           --\n",
      "│    │    └─AttentionDownSample: 3-6          [2, 8, 64, 64]            64\n",
      "│    │    └─Conv2d: 3-7                       [2, 16, 64, 64]           128\n",
      "│    │    └─Mish: 3-8                         [2, 16, 64, 64]           --\n",
      "│    │    └─ResBlock: 3-9                     [2, 16, 64, 64]           1,664\n",
      "│    └─Conv2d: 2-4                            [2, 4, 64, 64]            64\n",
      "│    └─Sequential: 2-5                        [2, 32, 16, 16]           --\n",
      "│    │    └─AttentionDownSample: 3-10         [2, 16, 16, 16]           256\n",
      "│    │    └─Conv2d: 3-11                      [2, 32, 16, 16]           512\n",
      "│    │    └─Mish: 3-12                        [2, 32, 16, 16]           --\n",
      "│    │    └─ResBlock: 3-13                    [2, 32, 16, 16]           4,672\n",
      "│    └─Conv2d: 2-6                            [2, 8, 16, 16]            256\n",
      "│    └─Sequential: 2-7                        [2, 64, 8, 8]             --\n",
      "│    │    └─AttentionDownSample: 3-14         [2, 32, 8, 8]             1,024\n",
      "│    │    └─Conv2d: 3-15                      [2, 64, 8, 8]             2,048\n",
      "│    │    └─Mish: 3-16                        [2, 64, 8, 8]             --\n",
      "│    │    └─ResBlock: 3-17                    [2, 64, 8, 8]             13,056\n",
      "│    └─Conv2d: 2-8                            [2, 16, 8, 8]             1,024\n",
      "├─Decoder: 1-2                                [2, 8, 512, 512]          --\n",
      "│    └─Sequential: 2-9                        [2, 8, 512, 512]          --\n",
      "│    │    └─Conv2d: 3-18                      [2, 32, 512, 512]         928\n",
      "│    │    └─Mish: 3-19                        [2, 32, 512, 512]         --\n",
      "│    │    └─Conv2d: 3-20                      [2, 8, 512, 512]          264\n",
      "│    │    └─Mish: 3-21                        [2, 8, 512, 512]          --\n",
      "===============================================================================================\n",
      "Total params: 27,627\n",
      "Trainable params: 27,627\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.52\n",
      "===============================================================================================\n",
      "Input size (MB): 2.10\n",
      "Forward/backward pass size (MB): 646.48\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 648.69\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(layer,input_size=(2, 1, 512, 512)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
