{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a5adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Transform.Schedule\n",
    "\n",
    "from Configuration import Editor\n",
    "\n",
    "with Editor('Config') as Config:\n",
    "\n",
    "    # settings for dataset\n",
    "    Config.Dataset.ImagesRootPath = r'D:\\Dataset_Collection\\Cardiac_Catheterization\\train\\images'\n",
    "    Config.Dataset.MasksRootPath = r'D:\\Dataset_Collection\\Cardiac_Catheterization\\train\\masks'\n",
    "    # control input and output image format\n",
    "    Config.Dataset.IO.InputRGBImage = False\n",
    "    Config.Dataset.IO.NumWorkers = 0\n",
    "    Config.Dataset.IO.PinMemory = False\n",
    "    Config.Dataset.IO.PrefetchFactor = 2\n",
    "    Config.Dataset.IO.OutputDtype = 'float'\n",
    "    # uniformed preprocess\n",
    "    Config.Dataset.Preprocess.Version = 'v1'\n",
    "    #train dataset\n",
    "    Config.Dataset.Train.BatchSize = 1\n",
    "    Config.Dataset.Train.Transform.Combination.Version = 'v1'\n",
    "    Config.Dataset.Train.Transform.Schedule = 0.8\n",
    "    Config.Dataset.Train.Transform.Combination.Components = 'default'\n",
    "    Config.Dataset.Train.Transform.Combination.Params = 'default'\n",
    "    Config.Dataset.Train.Transform.Combination.Schedules = 'default'\n",
    "    #validation dataset\n",
    "    Config.Dataset.Validation.BatchSize = 2\n",
    "    Config.Dataset.Validation.Ratio = 0.05\n",
    "    Config.Dataset.Validation.Transform.Combination.Version = 'v1'\n",
    "    Config.Dataset.Validation.Transform.Schedule = 0.8\n",
    "    Config.Dataset.Validation.Transform.Combination.Components = 'default'\n",
    "    Config.Dataset.Validation.Transform.Combination.Params = 'default'\n",
    "    Config.Dataset.Validation.Transform.Combination.Schedules = 'default'\n",
    "    \n",
    "    #choose training structure, including the model, loss, metrics, optimizer, schedular\n",
    "    Config.Training.Structure.Type = 'SimpleSeg'\n",
    "    # model\n",
    "    Config.Training.Structure.Model.Backbone.Name = 'Backbone1'\n",
    "    Config.Training.Structure.Model.Backbone.Param = dict()\n",
    "    Config.Training.Structure.Model.Head.Name = 'V1'\n",
    "    Config.Training.Structure.Model.Head.Param = dict(logit_output=True,in_channels=4)\n",
    "    # loss\n",
    "    Config.Training.Structure.Loss.Name = 'DiceBCELoss'\n",
    "    Config.Training.Structure.Loss.Param = dict(use_logit=True)\n",
    "    # optimizer\n",
    "    Config.Training.Structure.Optimizer.Name = 'Adam'\n",
    "    Config.Training.Structure.Optimizer.Param = dict(lr=0.0005)\n",
    "    # scheduler\n",
    "    Config.Training.Structure.Scheduler.Name = 'LinearLR'\n",
    "    Config.Training.Structure.Scheduler.Param = dict(warmup_epochs=2)\n",
    "    # metrics\n",
    "    Config.Training.Structure.Metrics.Name = ['DiceBCELoss']\n",
    "    Config.Training.Structure.Metrics.Param = [dict(use_logit=True)]\n",
    "\n",
    "    # setup training\n",
    "    Config.Training.Checkpoint.Path = None\n",
    "    Config.Training.Checkpoint.FileName = ''\n",
    "    Config.Training.Checkpoint.Resume.Process = False\n",
    "    Config.Training.Checkpoint.Resume.Optimizer = False\n",
    "    Config.Training.Checkpoint.Resume.Scheduler = False\n",
    "    \n",
    "    # whether to freeze backbone\n",
    "    Config.Training.Settings.Model.FreezeBackbone = True\n",
    "    \n",
    "    # setup training process\n",
    "    Config.Training.Settings.Epochs = 20\n",
    "    Config.Training.Settings.GradientAccumulation = 4\n",
    "    Config.Training.Settings.AmpScaleTrain = True\n",
    "    # set random property\n",
    "    Config.Training.Settings.Random.cuDNN.Deterministic = True\n",
    "    Config.Training.Settings.Random.cuDNN.Benchmark = True\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Split = 4\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Transform = 10\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Shuffle = 6\n",
    "    Config.Training.Settings.Random.Seed.Model = 99\n",
    "    \n",
    "    Config.Logging.StepsPerLog = 8\n",
    "    Config.Logging.Image.Columns = 3\n",
    "    Config.Logging.Image.Rows = 10\n",
    "    Config.Logging.Image.Figsize = (300,300)\n",
    "    Config.Logging.Image.Fontsize = 200\n",
    "    Config.Logging.Image.DPI = 10\n",
    "    Config.Logging.Image.MaskAlpha = 0.6\n",
    "    \n",
    "    Config.Logging.RootPath = 'logging'\n",
    "    Config.Logging.Model.Reference = 'Resnet'\n",
    "    Config.Logging.Model.Derivative = 'WithRegionalAttention'\n",
    "    Config.Logging.Model.Branch = '01'\n",
    "    Config.Logging.Comment = 'None'\n",
    "    Config.Logging.Purpose = 'None'\n",
    "    Config.Logging.Note = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a18b597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 Training:   0%|          | 0/1631 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-84d1a8a4cac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;31m#     recorder.update_metrics_state(purpose='train', predict=predicts, label=masks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamp_scale_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mrecorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_metrics_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m# update process state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Portfolio\\Cardiac_Catheterization_Project\\pytorch\\utils\\train.py\u001b[0m in \u001b[0;36mupdate_metrics_state\u001b[1;34m(self, purpose, predict, label)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_metrics_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlog_metrics_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Portfolio\\Cardiac_Catheterization_Project\\pytorch\\utils\\metric.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, predict, label)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mbatch_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc_value\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Portfolio\\Cardiac_Catheterization_Project\\pytorch\\utils\\loss.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, predict, label)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reduce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Portfolio\\Cardiac_Catheterization_Project\\pytorch\\Structure\\SimpleSeg\\Losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, predict, label)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mBCE_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mBCE_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mBCE_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBCE_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast."
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pathlib\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import Configuration\n",
    "from Dataset import TrainDataLoader\n",
    "import Transform.Preprocess\n",
    "import Transform.Combinations\n",
    "import Transform.Schedule\n",
    "import Structure\n",
    "import utils.train\n",
    "import utils.schedulers\n",
    "\n",
    "\n",
    "Config = Configuration.Config\n",
    "\n",
    "# create gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# cudnn reproducibility\n",
    "torch.backends.cudnn.deterministic = Config.Training.Settings.Random.cuDNN.Deterministic\n",
    "torch.backends.cudnn.benchmark = Config.Training.Settings.Random.cuDNN.Benchmark\n",
    "\n",
    "# create dataset: train_dataset, validation_dataset, validation_dataset_wo_arg\n",
    "# train_transform for dataloader\n",
    "train_transform_creater = getattr(Transform.Combinations, Config.Dataset.Train.Transform.Combination.Version)\n",
    "train_transform_creater = train_transform_creater(\n",
    "    Config.Dataset.Train.Transform.Schedule,\n",
    "    Config.Dataset.Train.Transform.Combination.Components,\n",
    "    Config.Dataset.Train.Transform.Combination.Params,\n",
    "    Config.Dataset.Train.Transform.Combination.Schedules)\n",
    "# validation_transform for dataloader\n",
    "validation_transform_creater = getattr(Transform.Combinations, Config.Dataset.Validation.Transform.Combination.Version)\n",
    "validation_transform_creater = validation_transform_creater(\n",
    "    Config.Dataset.Validation.Transform.Schedule,\n",
    "    Config.Dataset.Validation.Transform.Combination.Components,\n",
    "    Config.Dataset.Validation.Transform.Combination.Params,\n",
    "    Config.Dataset.Validation.Transform.Combination.Schedules)\n",
    "# standard preprocess operation for dataloader\n",
    "preprocess = getattr(Transform.Preprocess,Config.Dataset.Preprocess.Version)\n",
    "# create dataloader\n",
    "dataloader = TrainDataLoader(\n",
    "    images_root = Config.Dataset.ImagesRootPath,\n",
    "    masks_root = Config.Dataset.MasksRootPath,\n",
    "    train_transform = train_transform_creater,\n",
    "    train_batch_size = Config.Dataset.Train.BatchSize,\n",
    "    validation_ratio = Config.Dataset.Validation.Ratio,\n",
    "    validation_transform = validation_transform_creater,\n",
    "    validation_batch_size = Config.Dataset.Validation.BatchSize,\n",
    "    image_rgb = Config.Dataset.IO.InputRGBImage,\n",
    "    preprocess = preprocess,\n",
    "    num_workers = Config.Dataset.IO.NumWorkers,\n",
    "    pin_memory = Config.Dataset.IO.PinMemory,\n",
    "    prefetch_factor = Config.Dataset.IO.PrefetchFactor,\n",
    "    dtype=Config.Dataset.IO.OutputDtype)\n",
    "# get datasets from dataloader\n",
    "# seed for dataset\n",
    "dataset_transform_seed = Config.Training.Settings.Random.Seed.Dataset.Transform\n",
    "random.seed(dataset_transform_seed)\n",
    "dataset_split_seed = Config.Training.Settings.Random.Seed.Dataset.Split\n",
    "np.random.seed(dataset_split_seed)\n",
    "train_dataset, validation_dataset, validation_dataset_wo_arg = dataloader.get_dataset()\n",
    "\n",
    "# calcualte how many steps in an epoch and scheduler need this\n",
    "train_data_count = len(train_dataset)\n",
    "gradient_accumulation = Config.Training.Settings.GradientAccumulation\n",
    "train_batch_size = Config.Dataset.Train.BatchSize\n",
    "step_size =  gradient_accumulation * train_batch_size\n",
    "steps_per_epoch = int(math.ceil(train_data_count/step_size))\n",
    "\n",
    "# Select the training structure and corresponding to model, loss_fn, metrics\n",
    "structure = getattr(Structure,Config.Training.Structure.Type)\n",
    "# seed for model creation\n",
    "model_seed = Config.Training.Settings.Random.Seed.Model\n",
    "torch.manual_seed(model_seed)\n",
    "# start building the model\n",
    "# build the model backbone\n",
    "model_backbone_class = getattr(structure.Model.Backbones,Config.Training.Structure.Model.Backbone.Name)\n",
    "model_backbone_param = Config.Training.Structure.Model.Backbone.Param\n",
    "model_backbone = model_backbone_class(**model_backbone_param)\n",
    "# build the model head\n",
    "model_head_class = getattr(structure.Model.Heads,Config.Training.Structure.Model.Head.Name)\n",
    "model_head_param = Config.Training.Structure.Model.Head.Param\n",
    "model_head = model_head_class(**model_head_param)\n",
    "# create the model\n",
    "model = utils.train.ModelBuilder(model_backbone,model_head)\n",
    "model = model.to(device)\n",
    "# setup loss_fn\n",
    "loss_fn_class = getattr(structure.Losses,Config.Training.Structure.Loss.Name)\n",
    "loss_fn = loss_fn_class(**Config.Training.Structure.Loss.Param)\n",
    "# setup optimizer\n",
    "optimizer_class = getattr(torch.optim,Config.Training.Structure.Optimizer.Name)\n",
    "optimizer = optimizer_class(model.parameters(),**Config.Training.Structure.Optimizer.Param)\n",
    "# scheduler\n",
    "scheduler_class = getattr(utils.schedulers,Config.Training.Structure.Scheduler.Name)\n",
    "scheduler = scheduler_class(steps_per_epoch=steps_per_epoch,**Config.Training.Structure.Scheduler.Param)\n",
    "scheduler = scheduler(optimizer)\n",
    "# setup metrics and setup recoder\n",
    "metrics_class = [getattr(structure.Metrics,m) for m in Config.Training.Structure.Metrics.Name]\n",
    "metrics_params = Config.Training.Structure.Metrics.Param\n",
    "\n",
    "# initialize process state or resume checkpoint\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "images_count = 0\n",
    "checkpoint_path = Config.Training.Checkpoint.Path\n",
    "checkpoint_file = Config.Training.Checkpoint.FileName\n",
    "steps_per_log = Config.Logging.StepsPerLog\n",
    "if (checkpoint_path != None) or (checkpoint_path == ''):\n",
    "    checkpoint_file_path = pathlib.Path(checkpoint_path,checkpoint_file).as_posix()\n",
    "    checkpoint = torch.load(checkpoint_file_path)\n",
    "    \n",
    "    model_state_dict = checkpoint['model']\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    images_count = checkpoint['images_count']\n",
    "    global_step = checkpoint['global_step']\n",
    "    \n",
    "    if Config.Training.Checkpoint.Resume.Process:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    if Config.Training.Checkpoint.Resume.Optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "    if Config.Training.Checkpoint.Resume.Scheduler:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    \n",
    "# if the model's backbone is not going to be trained, but still keep the input and the output layers trainable. \n",
    "if Config.Training.Settings.Model.FreezeBackbone:\n",
    "    model.freeze_backbone()\n",
    "\n",
    "# setup for logging validation image segmentation examples\n",
    "ncols = Config.Logging.Image.Columns * 4\n",
    "nrows = Config.Logging.Image.Rows\n",
    "figsize = Config.Logging.Image.Figsize\n",
    "fontsize = Config.Logging.Image.Fontsize\n",
    "dpi = Config.Logging.Image.DPI\n",
    "mask_alpha = Config.Logging.Image.MaskAlpha\n",
    "\n",
    "amp_scale_train = Config.Training.Settings.AmpScaleTrain\n",
    "# if amp_scale_train:\n",
    "#     scaler = GradScaler()\n",
    "scaler = GradScaler(enabled=amp_scale_train)\n",
    "\n",
    "# seed for dataset shuffle\n",
    "dataset_shuffle_seed = Config.Training.Settings.Random.Seed.Dataset.Shuffle\n",
    "torch.manual_seed(dataset_shuffle_seed)\n",
    "\n",
    "# create recorder\n",
    "recorder = utils.train.Recorder(\n",
    "    root = Config.Logging.RootPath,\n",
    "    reference = Config.Logging.Model.Reference,\n",
    "    derivative = Config.Logging.Model.Derivative,\n",
    "    branch = Config.Logging.Model.Branch,\n",
    "    comment = Config.Logging.Comment,\n",
    "    purpose = Config.Logging.Purpose,\n",
    "    dataset_split_seed = dataset_split_seed,\n",
    "    dataset_transform_seed = dataset_transform_seed,\n",
    "    dataset_shuffle_seed = dataset_shuffle_seed,\n",
    "    model_seed = model_seed)\n",
    "recorder.create_metrics_and_writers(metrics_class,metrics_params)\n",
    "recorder.log_config(Config)\n",
    "\n",
    "# start training process\n",
    "end_epoch = start_epoch + Config.Training.Settings.Epochs\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    acc_count = 0\n",
    "    acc_loss = 0\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    for batch_train_data in train_dataset(epoch):\n",
    "        images,masks = batch_train_data\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # if amp_scale_train:\n",
    "        #     with autocast():\n",
    "        #         predicts = model(images)\n",
    "        #         loss = loss_fn(predicts,masks)\n",
    "        # else:\n",
    "        #     predicts = model(images)\n",
    "        #     loss = loss_fn(predicts,masks)\n",
    "        with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "            predicts = model(images)\n",
    "            loss = loss_fn(predicts,masks)\n",
    "            \n",
    "        # if amp_scale_train:\n",
    "        #     with autocast():\n",
    "        #         recorder.update_metrics_state(purpose='train', predict=predicts, label=masks)\n",
    "        # else:\n",
    "        #     recorder.update_metrics_state(purpose='train', predict=predicts, label=masks)\n",
    "        with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "            recorder.update_metrics_state(purpose='train', predict=predicts, label=masks)\n",
    "        \n",
    "        # update process state\n",
    "        images_count += images.shape[0]\n",
    "        acc_loss += loss\n",
    "        acc_count += 1\n",
    "    \n",
    "        # if it's time to do back propagation\n",
    "        if acc_count == gradient_accumulation:\n",
    "            acc_loss = acc_loss / gradient_accumulation\n",
    "            \n",
    "            # if amp_scale_train:\n",
    "            #     scaler.scale(acc_loss).backward()\n",
    "            #     scaler.step(optimizer)\n",
    "            #     scaler.update()\n",
    "            # else:\n",
    "            #     acc_loss.backward()\n",
    "            #     optimizer.step()\n",
    "\n",
    "            scaler.scale(acc_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            acc_loss = 0\n",
    "            acc_count = 0\n",
    "            global_step += 1\n",
    "        \n",
    "            if (global_step % steps_per_log == 0) and global_step != 0:\n",
    "                recorder.log_metrics_state(purpose='train',images_count=images_count)\n",
    "                recorder.reset_metrics_state(purpose='train')\n",
    "                recorder.log_lr(lr=optimizer.param_groups[0]['lr'],images_count=images_count)\n",
    "    \n",
    "    \n",
    "    if global_step % steps_per_log != 0:\n",
    "        recorder.log_metrics_state(purpose='train',images_count=images_count)\n",
    "        recorder.reset_metrics_state(purpose='train')\n",
    "        recorder.log_lr(lr=optimizer.param_groups[0]['lr'],images_count=images_count)\n",
    "    \n",
    "    if acc_count != 0:\n",
    "        acc_loss = acc_loss / acc_count\n",
    "        # if amp_scale_train:\n",
    "        #     scaler.scale(acc_loss).backward()\n",
    "        #     scaler.step(optimizer)\n",
    "        #     scaler.update()\n",
    "        # else:\n",
    "        #     acc_loss.backward()\n",
    "        #     optimizer.step()\n",
    "        scaler.scale(acc_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        acc_loss = 0\n",
    "        acc_count = 0\n",
    "        global_step += 1\n",
    "    \n",
    "    model.eval()\n",
    "    for purpose, val_dataset in zip(('validation','validation_wo_arg'),(validation_dataset, validation_dataset_wo_arg)):\n",
    "        \n",
    "        acc_output_images = 1\n",
    "        fig = plt.figure(figsize=figsize,dpi=dpi)\n",
    "        plt.axis(False)\n",
    "        \n",
    "        for batch_val_data in val_dataset(epoch):\n",
    "            images, masks = batch_val_data\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "                intermediate_predicts = model(images)\n",
    "                final_predicts = model.predict(images)\n",
    "            \n",
    "            if acc_output_images <= nrows*ncols:\n",
    "                for image,mask,predict in zip(images.cpu().numpy().squeeze(axis=1),masks.cpu().numpy(),final_predicts.cpu().detach().numpy()):\n",
    "                    if acc_output_images < nrows*ncols:\n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.title('Input Image',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.imshow(mask,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Mask',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.imshow(predict,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Predict',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(predict,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Predict',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "            \n",
    "            recorder.update_metrics_state(purpose=purpose, predict=intermediate_predicts, label=masks)\n",
    "        \n",
    "        scheduler.epoch(recorder)\n",
    "        \n",
    "        recorder.log_metrics_state(purpose=purpose,images_count=images_count)\n",
    "        checkpoint = dict(\n",
    "            model = model.state_dict(),\n",
    "            optimizer = optimizer.state_dict(),\n",
    "            scheduler = scheduler.state_dict(),\n",
    "            epoch = epoch,\n",
    "            global_step = global_step,\n",
    "            images_count = images_count)\n",
    "        recorder.save_checkpoint(purpose=purpose,checkpoint=checkpoint)\n",
    "        recorder.reset_metrics_state(purpose='train')\n",
    "        \n",
    "        #record image\n",
    "        plt.tight_layout()\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='raw', dpi=dpi)\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        img_arr = np.reshape(np.frombuffer(buf.getvalue(), dtype=np.uint8),\n",
    "                            newshape=(int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), -1))\n",
    "        buf.close()\n",
    "        recorder.log_image(purpose=purpose,image=img_arr,epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc802dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
