{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a5adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Transform.Schedule\n",
    "\n",
    "from Configuration import Editor\n",
    "\n",
    "with Editor('Config') as Config:\n",
    "\n",
    "    # settings for dataset\n",
    "    Config.Dataset.ImagesRootPath = r'D:\\Dataset_Collection\\Cardiac_Catheterization\\train\\images'\n",
    "    Config.Dataset.MasksRootPath = r'D:\\Dataset_Collection\\Cardiac_Catheterization\\train\\masks'\n",
    "    # control input and output image format\n",
    "    Config.Dataset.IO.InputRGBImage = False\n",
    "    Config.Dataset.IO.NumWorkers = 0\n",
    "    Config.Dataset.IO.PinMemory = False\n",
    "    Config.Dataset.IO.PrefetchFactor = 2\n",
    "    Config.Dataset.IO.OutputDtype = 'float'\n",
    "    # uniformed preprocess\n",
    "    Config.Dataset.Preprocess.Version = 'v1'\n",
    "    #train dataset\n",
    "    Config.Dataset.Train.BatchSize = 1\n",
    "    Config.Dataset.Train.Transform.Combination.Version = 'v1'\n",
    "    Config.Dataset.Train.Transform.Schedule = 0.8\n",
    "    Config.Dataset.Train.Transform.Combination.Components = 'default'\n",
    "    Config.Dataset.Train.Transform.Combination.Params = 'default'\n",
    "    Config.Dataset.Train.Transform.Combination.Schedules = 'default'\n",
    "    #validation dataset\n",
    "    Config.Dataset.Validation.BatchSize = 2\n",
    "    Config.Dataset.Validation.Ratio = 0.05\n",
    "    Config.Dataset.Validation.Transform.Combination.Version = 'v1'\n",
    "    Config.Dataset.Validation.Transform.Schedule = 0.8\n",
    "    Config.Dataset.Validation.Transform.Combination.Components = 'default'\n",
    "    Config.Dataset.Validation.Transform.Combination.Params = 'default'\n",
    "    Config.Dataset.Validation.Transform.Combination.Schedules = 'default'\n",
    "    \n",
    "    #choose training structure, including the model, loss, metrics, optimizer, schedular\n",
    "    Config.Training.Structure.Type = 'SimpleSeg'\n",
    "    # model\n",
    "    Config.Training.Structure.Model.Backbone.Name = 'Backbone1'\n",
    "    Config.Training.Structure.Model.Backbone.Param = dict(ues_instance_norm=True)\n",
    "    Config.Training.Structure.Model.Head.Name = 'V1'\n",
    "    Config.Training.Structure.Model.Head.Param = dict(logit_output=True,in_channels=4)\n",
    "    # loss\n",
    "    Config.Training.Structure.Loss.Name = 'DiceBCELoss'\n",
    "    Config.Training.Structure.Loss.Param = dict(use_logit=True,w_bce=0.2)\n",
    "    # optimizer\n",
    "    Config.Training.Structure.Optimizer.Name = 'Adam'\n",
    "    Config.Training.Structure.Optimizer.Param = dict(lr=0.001)\n",
    "    # scheduler\n",
    "    Config.Training.Structure.Scheduler.Name = 'CustomSchedule1'\n",
    "    Config.Training.Structure.Scheduler.Param = dict(warmup_epochs=1,reduce_gamma=-2)\n",
    "    # metrics\n",
    "    Config.Training.Structure.Metrics.Name = ['DiceBCELoss']\n",
    "    Config.Training.Structure.Metrics.Param = [dict(use_logit=True,w_bce=0.2)]\n",
    "\n",
    "    # setup training\n",
    "    Config.Training.Checkpoint.Path = None\n",
    "    Config.Training.Checkpoint.FileName = ''\n",
    "    Config.Training.Checkpoint.Resume.Process = False\n",
    "    Config.Training.Checkpoint.Resume.Optimizer = False\n",
    "    Config.Training.Checkpoint.Resume.Scheduler = False\n",
    "    \n",
    "    # whether to freeze backbone\n",
    "    Config.Training.Settings.Model.FreezeBackbone = True\n",
    "    \n",
    "    # setup training process\n",
    "    Config.Training.Settings.Epochs = 20\n",
    "    Config.Training.Settings.GradientAccumulation = 4\n",
    "    Config.Training.Settings.AmpScaleTrain = True\n",
    "    # set random property\n",
    "    Config.Training.Settings.Random.cuDNN.Deterministic = True\n",
    "    Config.Training.Settings.Random.cuDNN.Benchmark = True\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Split = 4\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Transform = 10\n",
    "    Config.Training.Settings.Random.Seed.Dataset.Shuffle = 6\n",
    "    Config.Training.Settings.Random.Seed.Model = 99\n",
    "    \n",
    "    Config.Logging.StepsPerLog = 8\n",
    "    Config.Logging.Image.Columns = 3\n",
    "    Config.Logging.Image.Rows = 10\n",
    "    Config.Logging.Image.Figsize = (300,300)\n",
    "    Config.Logging.Image.Fontsize = 200\n",
    "    Config.Logging.Image.DPI = 10\n",
    "    Config.Logging.Image.MaskAlpha = 0.6\n",
    "    \n",
    "    Config.Logging.RootPath = 'logging'\n",
    "    Config.Logging.Model.Reference = 'Resnet'\n",
    "    Config.Logging.Model.Derivative = 'WithRegionalAttention'\n",
    "    Config.Logging.Model.Branch = 'instance_norm02'\n",
    "    Config.Logging.Comment = 'DiceBCELoss'\n",
    "    Config.Logging.Purpose = 'None'\n",
    "    Config.Logging.Note = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a18b597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 Training: 100%|██████████| 1631/1631 [05:53<00:00,  4.61it/s]\n",
      "EPOCH: 0 Validation: 100%|██████████| 43/43 [00:14<00:00,  3.03it/s]\n",
      "EPOCH: 0 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.42it/s]\n",
      "EPOCH: 1 Training: 100%|██████████| 1631/1631 [05:40<00:00,  4.80it/s]\n",
      "EPOCH: 1 Validation: 100%|██████████| 43/43 [00:13<00:00,  3.09it/s]\n",
      "EPOCH: 1 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.96it/s]\n",
      "EPOCH: 2 Training: 100%|██████████| 1631/1631 [05:15<00:00,  5.17it/s]\n",
      "EPOCH: 2 Validation: 100%|██████████| 43/43 [00:14<00:00,  3.06it/s]\n",
      "EPOCH: 2 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.59it/s]\n",
      "EPOCH: 3 Training: 100%|██████████| 1631/1631 [05:14<00:00,  5.19it/s]\n",
      "EPOCH: 3 Validation: 100%|██████████| 43/43 [00:12<00:00,  3.41it/s]\n",
      "EPOCH: 3 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.32it/s]\n",
      "EPOCH: 4 Training: 100%|██████████| 1631/1631 [05:19<00:00,  5.11it/s]\n",
      "EPOCH: 4 Validation: 100%|██████████| 43/43 [00:12<00:00,  3.48it/s]\n",
      "EPOCH: 4 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.69it/s]\n",
      "EPOCH: 5 Training: 100%|██████████| 1631/1631 [05:10<00:00,  5.25it/s]\n",
      "EPOCH: 5 Validation: 100%|██████████| 43/43 [00:14<00:00,  3.02it/s]\n",
      "EPOCH: 5 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.99it/s]\n",
      "EPOCH: 6 Training: 100%|██████████| 1631/1631 [05:14<00:00,  5.19it/s]\n",
      "EPOCH: 6 Validation: 100%|██████████| 43/43 [00:11<00:00,  3.59it/s]\n",
      "EPOCH: 6 Validation Without Transform: 100%|██████████| 43/43 [00:07<00:00,  6.01it/s]\n",
      "EPOCH: 7 Training: 100%|██████████| 1631/1631 [05:17<00:00,  5.14it/s]\n",
      "EPOCH: 7 Validation: 100%|██████████| 43/43 [00:14<00:00,  3.04it/s]\n",
      "EPOCH: 7 Validation Without Transform: 100%|██████████| 43/43 [00:06<00:00,  6.95it/s]\n",
      "EPOCH: 8 Training:  80%|███████▉  | 1298/1631 [04:08<01:06,  5.03it/s]"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pathlib\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import Configuration\n",
    "from Dataset import TrainDataLoader\n",
    "import Transform.Preprocess\n",
    "import Transform.Combinations\n",
    "import Transform.Schedule\n",
    "import Structure\n",
    "import utils.train\n",
    "import utils.schedulers\n",
    "\n",
    "\n",
    "Config = Configuration.Config\n",
    "\n",
    "# create gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# cudnn reproducibility\n",
    "torch.backends.cudnn.deterministic = Config.Training.Settings.Random.cuDNN.Deterministic\n",
    "torch.backends.cudnn.benchmark = Config.Training.Settings.Random.cuDNN.Benchmark\n",
    "\n",
    "# create dataset: train_dataset, validation_dataset, validation_dataset_wo_arg\n",
    "# train_transform for dataloader\n",
    "train_transform_creater = getattr(Transform.Combinations, Config.Dataset.Train.Transform.Combination.Version)\n",
    "train_transform_creater = train_transform_creater(\n",
    "    Config.Dataset.Train.Transform.Schedule,\n",
    "    Config.Dataset.Train.Transform.Combination.Components,\n",
    "    Config.Dataset.Train.Transform.Combination.Params,\n",
    "    Config.Dataset.Train.Transform.Combination.Schedules)\n",
    "# validation_transform for dataloader\n",
    "validation_transform_creater = getattr(Transform.Combinations, Config.Dataset.Validation.Transform.Combination.Version)\n",
    "validation_transform_creater = validation_transform_creater(\n",
    "    Config.Dataset.Validation.Transform.Schedule,\n",
    "    Config.Dataset.Validation.Transform.Combination.Components,\n",
    "    Config.Dataset.Validation.Transform.Combination.Params,\n",
    "    Config.Dataset.Validation.Transform.Combination.Schedules)\n",
    "# standard preprocess operation for dataloader\n",
    "preprocess = getattr(Transform.Preprocess,Config.Dataset.Preprocess.Version)\n",
    "# create dataloader\n",
    "dataloader = TrainDataLoader(\n",
    "    images_root = Config.Dataset.ImagesRootPath,\n",
    "    masks_root = Config.Dataset.MasksRootPath,\n",
    "    train_transform = train_transform_creater,\n",
    "    train_batch_size = Config.Dataset.Train.BatchSize,\n",
    "    validation_ratio = Config.Dataset.Validation.Ratio,\n",
    "    validation_transform = validation_transform_creater,\n",
    "    validation_batch_size = Config.Dataset.Validation.BatchSize,\n",
    "    image_rgb = Config.Dataset.IO.InputRGBImage,\n",
    "    preprocess = preprocess,\n",
    "    num_workers = Config.Dataset.IO.NumWorkers,\n",
    "    pin_memory = Config.Dataset.IO.PinMemory,\n",
    "    prefetch_factor = Config.Dataset.IO.PrefetchFactor,\n",
    "    dtype=Config.Dataset.IO.OutputDtype)\n",
    "# get datasets from dataloader\n",
    "# seed for dataset\n",
    "dataset_transform_seed = Config.Training.Settings.Random.Seed.Dataset.Transform\n",
    "random.seed(dataset_transform_seed)\n",
    "dataset_split_seed = Config.Training.Settings.Random.Seed.Dataset.Split\n",
    "np.random.seed(dataset_split_seed)\n",
    "train_dataset, validation_dataset, validation_dataset_wo_arg = dataloader.get_dataset()\n",
    "\n",
    "# calcualte how many steps in an epoch and scheduler need this\n",
    "train_data_count = len(train_dataset)\n",
    "gradient_accumulation = Config.Training.Settings.GradientAccumulation\n",
    "train_batch_size = Config.Dataset.Train.BatchSize\n",
    "step_size =  gradient_accumulation * train_batch_size\n",
    "steps_per_epoch = int(math.ceil(train_data_count/step_size))\n",
    "\n",
    "# Select the training structure and corresponding to model, loss_fn, metrics\n",
    "structure = getattr(Structure,Config.Training.Structure.Type)\n",
    "# seed for model creation\n",
    "model_seed = Config.Training.Settings.Random.Seed.Model\n",
    "torch.manual_seed(model_seed)\n",
    "# start building the model\n",
    "# build the model backbone\n",
    "model_backbone_class = getattr(structure.Model.Backbones,Config.Training.Structure.Model.Backbone.Name)\n",
    "model_backbone_param = Config.Training.Structure.Model.Backbone.Param\n",
    "model_backbone = model_backbone_class(**model_backbone_param)\n",
    "# build the model head\n",
    "model_head_class = getattr(structure.Model.Heads,Config.Training.Structure.Model.Head.Name)\n",
    "model_head_param = Config.Training.Structure.Model.Head.Param\n",
    "model_head = model_head_class(**model_head_param)\n",
    "# create the model\n",
    "model = utils.train.ModelBuilder(model_backbone,model_head)\n",
    "model = model.to(device)\n",
    "# setup loss_fn\n",
    "loss_fn_class = getattr(structure.Losses,Config.Training.Structure.Loss.Name)\n",
    "loss_fn = loss_fn_class(**Config.Training.Structure.Loss.Param)\n",
    "# setup optimizer\n",
    "optimizer_class = getattr(torch.optim,Config.Training.Structure.Optimizer.Name)\n",
    "optimizer = optimizer_class(model.parameters(),**Config.Training.Structure.Optimizer.Param)\n",
    "# scheduler\n",
    "scheduler_class = getattr(utils.schedulers,Config.Training.Structure.Scheduler.Name)\n",
    "scheduler = scheduler_class(steps_per_epoch=steps_per_epoch,**Config.Training.Structure.Scheduler.Param)\n",
    "scheduler = scheduler(optimizer)\n",
    "# setup metrics and setup recoder\n",
    "metrics_class = [getattr(structure.Metrics,m) for m in Config.Training.Structure.Metrics.Name]\n",
    "metrics_params = Config.Training.Structure.Metrics.Param\n",
    "\n",
    "# initialize process state or resume checkpoint\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "images_count = 0\n",
    "checkpoint_path = Config.Training.Checkpoint.Path\n",
    "checkpoint_file = Config.Training.Checkpoint.FileName\n",
    "steps_per_log = Config.Logging.StepsPerLog\n",
    "if (checkpoint_path != None) or (checkpoint_path == ''):\n",
    "    checkpoint_file_path = pathlib.Path(checkpoint_path,checkpoint_file).as_posix()\n",
    "    checkpoint = torch.load(checkpoint_file_path)\n",
    "    \n",
    "    model_state_dict = checkpoint['model']\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    images_count = checkpoint['images_count']\n",
    "    global_step = checkpoint['global_step']\n",
    "    \n",
    "    if Config.Training.Checkpoint.Resume.Process:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    if Config.Training.Checkpoint.Resume.Optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "    if Config.Training.Checkpoint.Resume.Scheduler:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    \n",
    "# if the model's backbone is not going to be trained, but still keep the input and the output layers trainable. \n",
    "if Config.Training.Settings.Model.FreezeBackbone:\n",
    "    model.freeze_backbone()\n",
    "\n",
    "# setup for logging validation image segmentation examples\n",
    "ncols = Config.Logging.Image.Columns * 4\n",
    "nrows = Config.Logging.Image.Rows\n",
    "figsize = Config.Logging.Image.Figsize\n",
    "fontsize = Config.Logging.Image.Fontsize\n",
    "dpi = Config.Logging.Image.DPI\n",
    "mask_alpha = Config.Logging.Image.MaskAlpha\n",
    "\n",
    "amp_scale_train = Config.Training.Settings.AmpScaleTrain\n",
    "# if amp_scale_train:\n",
    "#     scaler = GradScaler()\n",
    "scaler = GradScaler(enabled=amp_scale_train)\n",
    "\n",
    "# seed for dataset shuffle\n",
    "dataset_shuffle_seed = Config.Training.Settings.Random.Seed.Dataset.Shuffle\n",
    "torch.manual_seed(dataset_shuffle_seed)\n",
    "\n",
    "# create recorder\n",
    "recorder = utils.train.Recorder(\n",
    "    root = Config.Logging.RootPath,\n",
    "    reference = Config.Logging.Model.Reference,\n",
    "    derivative = Config.Logging.Model.Derivative,\n",
    "    branch = Config.Logging.Model.Branch,\n",
    "    comment = Config.Logging.Comment,\n",
    "    purpose = Config.Logging.Purpose,\n",
    "    dataset_split_seed = dataset_split_seed,\n",
    "    dataset_transform_seed = dataset_transform_seed,\n",
    "    dataset_shuffle_seed = dataset_shuffle_seed,\n",
    "    model_seed = model_seed)\n",
    "recorder.create_metrics_and_writers(metrics_class,metrics_params)\n",
    "recorder.log_config(Config)\n",
    "\n",
    "# start training process\n",
    "end_epoch = start_epoch + Config.Training.Settings.Epochs\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    acc_count = 0\n",
    "    acc_loss = 0\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    for batch_train_data in train_dataset(epoch):\n",
    "        images,masks = batch_train_data\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "            predicts = model(images)\n",
    "            # loss = loss_fn(predicts,masks)\n",
    "            loss = loss_fn(predicts,masks)['loss']\n",
    "            recorder.update_metrics_state(purpose='train', predict=predicts, label=masks)\n",
    "\n",
    "        # update process state\n",
    "        images_count += images.shape[0]\n",
    "        acc_loss += loss\n",
    "        acc_count += 1\n",
    "    \n",
    "        # if it's time to do back propagation\n",
    "        if acc_count == gradient_accumulation:\n",
    "            acc_loss = acc_loss / gradient_accumulation\n",
    "            scaler.scale(acc_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            acc_loss = 0\n",
    "            acc_count = 0\n",
    "            global_step += 1\n",
    "        \n",
    "            if (global_step % steps_per_log == 0) and global_step != 0:\n",
    "                recorder.log_metrics_state(purpose='train',images_count=images_count)\n",
    "                recorder.reset_metrics_state(purpose='train')\n",
    "                recorder.log_lr(lr=optimizer.param_groups[0]['lr'],images_count=images_count)\n",
    "    \n",
    "    \n",
    "    if global_step % steps_per_log != 0:\n",
    "        recorder.log_metrics_state(purpose='train',images_count=images_count)\n",
    "        recorder.reset_metrics_state(purpose='train')\n",
    "        recorder.log_lr(lr=optimizer.param_groups[0]['lr'],images_count=images_count)\n",
    "    \n",
    "    if acc_count != 0:\n",
    "        acc_loss = acc_loss / acc_count\n",
    "        scaler.scale(acc_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        acc_loss = 0\n",
    "        acc_count = 0\n",
    "        global_step += 1\n",
    "    \n",
    "    model.eval()\n",
    "    for purpose, val_dataset in zip(('validation','validation_wo_arg'),(validation_dataset, validation_dataset_wo_arg)):\n",
    "        \n",
    "        acc_output_images = 1\n",
    "        fig = plt.figure(figsize=figsize,dpi=dpi)\n",
    "        plt.axis(False)\n",
    "        \n",
    "        for batch_val_data in val_dataset(epoch):\n",
    "            images, masks = batch_val_data\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "                with torch.no_grad():\n",
    "                    intermediate_predicts = model(images)\n",
    "                recorder.update_metrics_state(purpose=purpose, predict=intermediate_predicts, label=masks)\n",
    "            del intermediate_predicts\n",
    "            \n",
    "            with autocast(enabled=amp_scale_train,dtype=torch.float32):\n",
    "                final_predicts = model.predict(images)\n",
    "            if acc_output_images <= nrows*ncols:\n",
    "                for image,mask,predict in zip(images.cpu().numpy().squeeze(axis=1),masks.cpu().numpy(),final_predicts.cpu().detach().numpy()):\n",
    "                    if acc_output_images < nrows*ncols:\n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.title('Input Image',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.imshow(mask,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Mask',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(image,cmap='gray')\n",
    "                        plt.imshow(predict,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Predict',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "                        \n",
    "                        fig.add_subplot(nrows,ncols,acc_output_images)\n",
    "                        plt.imshow(predict,cmap='gray',alpha=mask_alpha)\n",
    "                        plt.title('Predict',fontsize=fontsize)\n",
    "                        plt.axis(False)\n",
    "                        acc_output_images += 1\n",
    "            del final_predicts\n",
    "            del images\n",
    "            del masks\n",
    "        \n",
    "        scheduler.epoch(recorder)\n",
    "        \n",
    "        recorder.log_metrics_state(purpose=purpose,images_count=images_count)\n",
    "        checkpoint = dict(\n",
    "            model = model.state_dict(),\n",
    "            optimizer = optimizer.state_dict(),\n",
    "            scheduler = scheduler.state_dict(),\n",
    "            epoch = epoch,\n",
    "            global_step = global_step,\n",
    "            images_count = images_count)\n",
    "        recorder.save_checkpoint(purpose=purpose,checkpoint=checkpoint)\n",
    "        recorder.reset_metrics_state(purpose='train')\n",
    "        \n",
    "        #record image\n",
    "        plt.tight_layout()\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='raw', dpi=dpi)\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        img_arr = np.reshape(np.frombuffer(buf.getvalue(), dtype=np.uint8),\n",
    "                            newshape=(int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), -1))\n",
    "        buf.close()\n",
    "        recorder.log_image(purpose=purpose,image=img_arr,epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc802dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
